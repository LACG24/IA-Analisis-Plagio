{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257db376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Archivo no encontrado: dataset/par-29-a-Treesort.py o dataset/par-29-e-Treesort.py\n",
      "\n",
      "Pares cargados: 122\n",
      "Distribuci√≥n de clases: Counter({1: 31, 3: 31, 4: 30, 2: 30})\n",
      "‚úÖ Usando stratify para dividir train/test\n",
      "\n",
      "=== Reporte de Clasificaci√≥n ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.78      0.58         9\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.12      0.10      0.11        10\n",
      "           4       0.33      0.44      0.38         9\n",
      "\n",
      "    accuracy                           0.32        37\n",
      "   macro avg       0.23      0.33      0.27        37\n",
      "weighted avg       0.23      0.32      0.26        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Inicializar CodeBERT\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Anonimizaci√≥n b√°sica\n",
    "def anonymizar_codigo(code):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n",
    "    usados = {}\n",
    "    nuevo_codigo = code\n",
    "    contador = 1\n",
    "    for tok in tokens:\n",
    "        if tok not in usados and tok not in {\"def\", \"if\", \"else\", \"for\", \"while\", \"return\", \"print\", \"input\"}:\n",
    "            usados[tok] = f\"VAR_{contador}\"\n",
    "            contador += 1\n",
    "    for original, nuevo in usados.items():\n",
    "        nuevo_codigo = re.sub(rf'\\b{original}\\b', nuevo, nuevo_codigo)\n",
    "    return nuevo_codigo\n",
    "\n",
    "# Extraer funci√≥n principal del archivo\n",
    "def extraer_funcion_principal(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        code = f.read()\n",
    "    funciones = re.findall(r\"(def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\):(?:\\n(?:\\s{4}|\\t).*)*)\", code)\n",
    "    return funciones[0] if funciones else code  # Si no hay funciones, usar todo\n",
    "\n",
    "# Embedding con CodeBERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Pipeline principal\n",
    "def entrenar_clasificador(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path1, path2, etiqueta = row[\"codigo_1\"], row[\"codigo_2\"], row[\"tipo_plagio\"]\n",
    "\n",
    "        if not os.path.exists(path1) or not os.path.exists(path2):\n",
    "            print(f\"‚ùå Archivo no encontrado: {path1} o {path2}\")\n",
    "            continue\n",
    "\n",
    "        cod1 = anonymizar_codigo(extraer_funcion_principal(path1))\n",
    "        cod2 = anonymizar_codigo(extraer_funcion_principal(path2))\n",
    "\n",
    "        emb1 = get_embedding(cod1)\n",
    "        emb2 = get_embedding(cod2)\n",
    "\n",
    "        vector = np.concatenate([emb1, emb2, np.abs(emb1 - emb2)])\n",
    "        X.append(vector)\n",
    "        y.append(etiqueta)\n",
    "\n",
    "    print(f\"\\nPares cargados: {len(X)}\")\n",
    "    print(\"Distribuci√≥n de clases:\", Counter(y))\n",
    "\n",
    "    if len(X) >= 4:\n",
    "        try:\n",
    "            if all(v >= 2 for v in Counter(y).values()):\n",
    "                print(\"‚úÖ Usando stratify para dividir train/test\")\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Pocas muestras en alguna clase. Usando split sin stratify.\")\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Error al dividir con stratify. Reintentando sin stratify.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== Reporte de Clasificaci√≥n ===\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay suficientes ejemplos para entrenar el modelo.\")\n",
    "\n",
    "# Ejecutar\n",
    "entrenar_clasificador(\"pares.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7aa5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç 2 funciones en dataset/par-10-a-bubble-sort.py\n",
      "üîç 1 funciones en dataset/prueba.cpp\n",
      "Similitud f0 (A) vs f0 (B): 0.9805\n",
      "Similitud f1 (A) vs f0 (B): 0.9850\n",
      "\n",
      "üßæ Matriz de similitud:\n",
      "[[0.9805]\n",
      " [0.985 ]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Inicializar CodeBERT ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# === Funci√≥n para anonimizar c√≥digo ===\n",
    "def anonimizar_codigo(code):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n",
    "    usados = {}\n",
    "    nuevo_codigo = code\n",
    "    contador = 1\n",
    "    for tok in tokens:\n",
    "        if tok not in usados and tok not in {\"def\", \"if\", \"else\", \"for\", \"while\", \"return\", \"print\", \"input\"}:\n",
    "            usados[tok] = f\"VAR_{contador}\"\n",
    "            contador += 1\n",
    "    for original, nuevo in usados.items():\n",
    "        nuevo_codigo = re.sub(rf'\\b{original}\\b', nuevo, nuevo_codigo)\n",
    "    return nuevo_codigo\n",
    "\n",
    "# === Extraer todas las funciones de un archivo ===\n",
    "def extraer_todas_funciones(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        code = f.read()\n",
    "    funciones = re.findall(r\"(def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\):(?:\\n(?:\\s{4}|\\t).*)*)\", code)\n",
    "    return funciones if funciones else [code]\n",
    "\n",
    "# === Obtener embedding de una funci√≥n ===\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# === Comparar funciones entre dos archivos ===\n",
    "def comparar_funciones(path1, path2):\n",
    "    funciones_1 = extraer_todas_funciones(path1)\n",
    "    funciones_2 = extraer_todas_funciones(path2)\n",
    "\n",
    "    print(f\"üîç {len(funciones_1)} funciones en {path1}\")\n",
    "    print(f\"üîç {len(funciones_2)} funciones en {path2}\")\n",
    "\n",
    "    emb_1 = [obtener_embedding(anonimizar_codigo(f)) for f in funciones_1]\n",
    "    emb_2 = [obtener_embedding(anonimizar_codigo(f)) for f in funciones_2]\n",
    "\n",
    "    similitudes = []\n",
    "    for i, e1 in enumerate(emb_1):\n",
    "        fila = []\n",
    "        for j, e2 in enumerate(emb_2):\n",
    "            sim = cosine_similarity([e1], [e2])[0][0]\n",
    "            fila.append(sim)\n",
    "            print(f\"Similitud f{i} (A) vs f{j} (B): {sim:.4f}\")\n",
    "        similitudes.append(fila)\n",
    "\n",
    "    return np.array(similitudes)\n",
    "\n",
    "# === Ejemplo de uso ===\n",
    "if __name__ == \"__main__\":\n",
    "    archivo_a = \"dataset/par-10-a-bubble-sort.py\"\n",
    "    archivo_b = \"dataset/prueba.cpp\"\n",
    "    matriz = comparar_funciones(archivo_a, archivo_b)\n",
    "\n",
    "    print(\"\\nüßæ Matriz de similitud:\")\n",
    "    print(np.round(matriz, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecdbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares v√°lidos: 237\n",
      "Distribuci√≥n: Counter({2: 71, 3: 70, 1: 66, 4: 30})\n",
      "\n",
      "=== Reporte de Clasificaci√≥n (KNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.70      0.61        20\n",
      "           2       0.32      0.32      0.32        22\n",
      "           3       0.61      0.52      0.56        21\n",
      "           4       0.33      0.22      0.27         9\n",
      "\n",
      "    accuracy                           0.47        72\n",
      "   macro avg       0.45      0.44      0.44        72\n",
      "weighted avg       0.47      0.47      0.46        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Aseg√∫rate de tener definidas estas funciones (ya las tienes):\n",
    "# - anonimizar_codigo()\n",
    "# - extraer_todas_funciones()\n",
    "# - obtener_embedding()\n",
    "\n",
    "def features_por_par(path1, path2):\n",
    "    if not os.path.exists(path1) or not os.path.exists(path2):\n",
    "        return None\n",
    "\n",
    "    funcs1 = extraer_todas_funciones(path1)\n",
    "    funcs2 = extraer_todas_funciones(path2)\n",
    "\n",
    "    emb1 = [obtener_embedding(anonimizar_codigo(f)) for f in funcs1]\n",
    "    emb2 = [obtener_embedding(anonimizar_codigo(f)) for f in funcs2]\n",
    "\n",
    "    if not emb1 or not emb2:\n",
    "        return None\n",
    "\n",
    "    # Cambia cosine_similarity por euclidean\n",
    "    distancias = [-euclidean(e1, e2) for e1 in emb1 for e2 in emb2]\n",
    "\n",
    "    max_sim = max(distancias)\n",
    "    avg_sim = np.mean(distancias)\n",
    "    count_95 = sum(1 for s in distancias if s > -0.05)\n",
    "    count_90 = sum(1 for s in distancias if s > -0.1)\n",
    "\n",
    "    return [max_sim, avg_sim, count_95, count_90, len(emb1), len(emb2)]\n",
    "\n",
    "\n",
    "def entrenar_modelo_funcion_por_funcion(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path1, path2, label = row[\"codigo_1\"], row[\"codigo_2\"], row[\"tipo_plagio\"]\n",
    "        feats = features_por_par(path1, path2)\n",
    "        if feats is not None:\n",
    "            X.append(feats)\n",
    "            y.append(label)\n",
    "\n",
    "    print(f\"Pares v√°lidos: {len(X)}\")\n",
    "    print(\"Distribuci√≥n:\", Counter(y))\n",
    "\n",
    "    if len(set(y)) > 1:\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "        except ValueError:\n",
    "            print(\"‚ö†Ô∏è Stratify no posible, dividiendo sin estratificaci√≥n.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        clf = KNeighborsClassifier(n_neighbors=15)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== Reporte de Clasificaci√≥n (KNN) ===\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Solo hay una clase en el dataset. No se puede entrenar.\")\n",
    "\n",
    "entrenar_modelo_funcion_por_funcion(\"pares.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
