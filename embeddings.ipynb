{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257db376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Inicializar CodeBERT\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Anonimizaci√≥n b√°sica\n",
    "def anonymizar_codigo(code):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n",
    "    usados = {}\n",
    "    nuevo_codigo = code\n",
    "    contador = 1\n",
    "    for tok in tokens:\n",
    "        if tok not in usados and tok not in {\"def\", \"if\", \"else\", \"for\", \"while\", \"return\", \"print\", \"input\"}:\n",
    "            usados[tok] = f\"VAR_{contador}\"\n",
    "            contador += 1\n",
    "    for original, nuevo in usados.items():\n",
    "        nuevo_codigo = re.sub(rf'\\b{original}\\b', nuevo, nuevo_codigo)\n",
    "    return nuevo_codigo\n",
    "\n",
    "# Extraer funci√≥n principal del archivo\n",
    "def extraer_funcion_principal(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        code = f.read()\n",
    "    funciones = re.findall(r\"(def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\):(?:\\n(?:\\s{4}|\\t).*)*)\", code)\n",
    "    return funciones[0] if funciones else code  # Si no hay funciones, usar todo\n",
    "\n",
    "# Embedding con CodeBERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Pipeline principal\n",
    "def entrenar_clasificador(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path1, path2, etiqueta = row[\"codigo_1\"], row[\"codigo_2\"], row[\"tipo_plagio\"]\n",
    "\n",
    "        if not os.path.exists(path1) or not os.path.exists(path2):\n",
    "            print(f\"‚ùå Archivo no encontrado: {path1} o {path2}\")\n",
    "            continue\n",
    "\n",
    "        cod1 = anonymizar_codigo(extraer_funcion_principal(path1))\n",
    "        cod2 = anonymizar_codigo(extraer_funcion_principal(path2))\n",
    "\n",
    "        emb1 = get_embedding(cod1)\n",
    "        emb2 = get_embedding(cod2)\n",
    "\n",
    "        vector = np.concatenate([emb1, emb2, np.abs(emb1 - emb2)])\n",
    "        X.append(vector)\n",
    "        y.append(etiqueta)\n",
    "\n",
    "    print(f\"\\nPares cargados: {len(X)}\")\n",
    "    print(\"Distribuci√≥n de clases:\", Counter(y))\n",
    "\n",
    "    if len(X) >= 4:\n",
    "        try:\n",
    "            if all(v >= 2 for v in Counter(y).values()):\n",
    "                print(\"‚úÖ Usando stratify para dividir train/test\")\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Pocas muestras en alguna clase. Usando split sin stratify.\")\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Error al dividir con stratify. Reintentando sin stratify.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"\\n=== Reporte de Clasificaci√≥n ===\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay suficientes ejemplos para entrenar el modelo.\")\n",
    "\n",
    "# Ejecutar\n",
    "entrenar_clasificador(\"pares.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7aa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Inicializar CodeBERT ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# === Funci√≥n para anonimizar c√≥digo ===\n",
    "def anonimizar_codigo(code):\n",
    "    tokens = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n",
    "    usados = {}\n",
    "    nuevo_codigo = code\n",
    "    contador = 1\n",
    "    for tok in tokens:\n",
    "        if tok not in usados and tok not in {\"def\", \"if\", \"else\", \"for\", \"while\", \"return\", \"print\", \"input\"}:\n",
    "            usados[tok] = f\"VAR_{contador}\"\n",
    "            contador += 1\n",
    "    for original, nuevo in usados.items():\n",
    "        nuevo_codigo = re.sub(rf'\\b{original}\\b', nuevo, nuevo_codigo)\n",
    "    return nuevo_codigo\n",
    "\n",
    "# === Extraer todas las funciones de un archivo ===\n",
    "def extraer_todas_funciones(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        code = f.read()\n",
    "    funciones = re.findall(r\"(def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\):(?:\\n(?:\\s{4}|\\t).*)*)\", code)\n",
    "    return funciones if funciones else [code]\n",
    "\n",
    "# === Obtener embedding de una funci√≥n ===\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# === Comparar funciones entre dos archivos ===\n",
    "def comparar_funciones(path1, path2):\n",
    "    funciones_1 = extraer_todas_funciones(path1)\n",
    "    funciones_2 = extraer_todas_funciones(path2)\n",
    "\n",
    "    print(f\"üîç {len(funciones_1)} funciones en {path1}\")\n",
    "    print(f\"üîç {len(funciones_2)} funciones en {path2}\")\n",
    "\n",
    "    emb_1 = [obtener_embedding(anonimizar_codigo(f)) for f in funciones_1]\n",
    "    emb_2 = [obtener_embedding(anonimizar_codigo(f)) for f in funciones_2]\n",
    "\n",
    "    similitudes = []\n",
    "    for i, e1 in enumerate(emb_1):\n",
    "        fila = []\n",
    "        for j, e2 in enumerate(emb_2):\n",
    "            sim = cosine_similarity([e1], [e2])[0][0]\n",
    "            fila.append(sim)\n",
    "            print(f\"Similitud f{i} (A) vs f{j} (B): {sim:.4f}\")\n",
    "        similitudes.append(fila)\n",
    "\n",
    "    return np.array(similitudes)\n",
    "\n",
    "# === Ejemplo de uso ===\n",
    "if __name__ == \"__main__\":\n",
    "    archivo_a = \"dataset/par-10-a-bubble-sort.py\"\n",
    "    archivo_b = \"dataset/par-10-b-bubble-sort.py\"\n",
    "    matriz = comparar_funciones(archivo_a, archivo_b)\n",
    "\n",
    "    print(\"\\nüßæ Matriz de similitud:\")\n",
    "    print(np.round(matriz, 4))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
